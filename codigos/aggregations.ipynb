{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"agg\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x20df7ee3a00>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x: x + 1, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../data/retail-data/all/*.csv\")\n",
    "    .coalesce(5)\n",
    ")\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(count(\"StockCode\").alias(\"count_stock_code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(count_distinct(\"StockCode\").alias(\"count_stock_code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(approx_count_distinct(\"StockCode\", 0.03).alias(\"count_stock_code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sum_distinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"),\n",
    ").selectExpr(\n",
    "    \"(total_purchases/total_transactions) as ratio_purchases_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.where(col(\"CustomerId\").isNotNull())\n",
    "    .groupBy(\"InvoiceNo\", \"CustomerId\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupBy(\"InvoiceNo\").agg(count_distinct(\"Quantity\").alias(\"quan\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pessoas = (\n",
    "    df.groupBy(\"InvoiceNo\")\n",
    "    .agg(sum(\"Quantity\").alias(\"sum_quantity\"))\n",
    "    .where(col(\"sum_quantity\") > 0)\n",
    "    .dropDuplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pessoas.agg(max(col(\"sum_quantity\")), min(col(\"sum_quantity\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(desc(\"sum_quantity\"))\n",
    "\n",
    "pessoas_10_pct = pessoas.withColumn(\"10_porcento\", ntile(10).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pessoas_10_pct.orderBy(desc(\"sum_quantity\")).show()\n",
    "\n",
    "\n",
    "pessoas_10_pct.where(col(\"10_porcento\") == 1).agg(min(col(\"sum_quantity\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_date = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"M/d/y H:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = (\n",
    "    Window.partitionBy(\"CustomerID\", \"date\")\n",
    "    .orderBy(desc(\"Quantity\"))\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "sumPurchase = sum((\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_date.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\").select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantity_rank\"),\n",
    "    purchaseDenseRank.alias(\"quantity_dense_rank\"),\n",
    "    maxPurchaseQuantity.alias(\"max_purchase_quantity\"),\n",
    "    sumPurchase.alias(\"sum_quantity\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = df_with_date.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted.filter('date > \"2011-12-05\"').select(\"date\", \"`USA_sum(Quantity)`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame(\n",
    "    [\n",
    "        (0, \"Bill Chambers\", 0, [100]),\n",
    "        (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "        (2, \"Michael Armbrust\", 1, [250, 100]),\n",
    "    ]\n",
    ").toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame(\n",
    "    [\n",
    "        (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "        (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "        (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\"),\n",
    "    ]\n",
    ").toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus = spark.createDataFrame(\n",
    "    [(500, \"Vice President\"), (250, \"PMC Member\"), (100, \"Contributor\")]\n",
    ").toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [StructField(\"DESC\", StringType(), False), StructField(\"ID\", IntegerType(), False)]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame([temp1], schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"Category\": \"A\", \"ID\": 1, \"Value\": 121.44, \"Truth\": True},\n",
    "    {\"Category\": \"B\", \"ID\": 2, \"Value\": 300.01, \"Truth\": False},\n",
    "    {\"Category\": \"C\", \"ID\": 3, \"Value\": 10.99, \"Truth\": None},\n",
    "    {\"Category\": \"E\", \"ID\": 4, \"Value\": 33.87, \"Truth\": False},\n",
    "]\n",
    "\n",
    "cdf = spark.createDataFrame(data)\n",
    "type(cdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spk_c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
